{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#meta 3/16/2021 Transformer Models\r\n",
        "\r\n",
        "#history\r\n",
        "#3/16/2021 TRANSFORMERS\r\n",
        "#      Create an env\r\n",
        "#      Try HF, Sentiment and Paraphrase\r\n",
        "\r\n",
        "#5/10/2021 TRY PARAPHRASE, SUMMARIZE\r\n",
        "#      Try WOS data"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029716418
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Tiny Examples"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline \r\n",
        "print(pipeline('sentiment-analysis')('we love you'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029727338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline('sentiment-analysis')('jeans'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.5292837023735046}]\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029733057
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2Model\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\n",
        "model = TFGPT2Model.from_pretrained('gpt2')\r\n",
        "text = \"Replace me by any text you'd like.\"\r\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\r\n",
        "output = model(encoded_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1616029770687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Classification\r\n",
        "https://huggingface.co/transformers/task_summary.html"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\r\n",
        "import tensorflow as tf\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_0 = \"The company HuggingFace is based in New York City\"\r\n",
        "sequence_1 = \"Apples are especially bad for your health\"\r\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\r\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n",
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029976763
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_0 = \"blue jeans\"\r\n",
        "sequence_1 = \"red apples\"\r\n",
        "sequence_2 = \"pants\"\r\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 41%\n",
            "is paraphrase: 59%\n",
            "not paraphrase: 78%\n",
            "is paraphrase: 22%\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029930517
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. WOS Data Examples"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1_cat = 'Electric motor'\r\n",
        "s1_kw = 'NdFeB magnets; Electric motor; Electric vehicle; Hybrid electric vehicle; Recycling; Rare earth elements'\r\n",
        "s1_abstract = 'Hybrid electric vehicles are assumed to play a major role in future mobility concepts. Although sales numbers are increasing, little emphasis has been laid on the recycling of some key components such as power electronics or electric motors. Permanent magnet synchronous motors contain considerable amounts of rare earth elements that cannot be recovered in conventional recycling routes. Although their recycling could have large economic, environmental, and strategic advantages, no industrial recycling for permanent magnets is available in western countries at the moment. Regarding the essential steps, dismantling of electric vehicles as well as the extraction of magnets from the rotors, little has been published before. This paper therefore presents and discusses different recycling approaches for the recycling of NdFeB magnets from (hybrid) electric vehicles. Many results stem from the German research project \"Recycling of components and strategic metals of electric drive motors.'\r\n",
        "\r\n",
        "s2_cat = \"Green Building\"\r\n",
        "s2_kw = \"LED lighting system; PV system; Distributed lighting control; Energy efficiency; Green building; Daylight responsive dimming system\"\r\n",
        "s2_abstract = \"Decreasing of energy consumption and environmentally friendly energy resources are the issues in the foreground nowadays. As the electric energy consumed for the illumination is high, long-lasting and low-consumption LED (light-emitting diode) technology gets prominent. There have been made much reseacrh regarding the use of photovoltaic sytems in meeting the energy demand in housing and industry. However, there is need for more research with regards to photovoltaic sytems' integration with energy efficiency sytems. In this study, for the environments which have different lighting levels due to daylight factor, there has been proposed a low-cost PV (photovoltaics) based and distributed sensor smart LED illuminating system and there has been acquired 72.075% more energy saving in comparison with conventional LED illuminating system.\""
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Try Paraphrase\r\n",
        "\r\n",
        "Compare abstracts and kws"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrase = tokenizer(s1_abstract, s1_kw, return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "not_paraphrase = tokenizer(s1_abstract, s2_kw, return_tensors=\"tf\")\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Try Summarize\r\n",
        "Summarize abstract and compare with a) abstract, b) kws"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1_abstract_summary = summarizer(s1_abstract, max_length=100, min_length=30, do_sample=False)\r\n",
        "s2_abstract_summary = summarizer(s2_abstract, max_length=100, min_length=30, do_sample=False)\r\n",
        "\r\n",
        "s1_abstract_summary #class list of dicts\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "[{'summary_text': ' Hybrid electric vehicles are assumed to play a major role in future mobility concepts . Little emphasis has been laid on the recycling of some key components such as power electronics or electric motors . No industrial recycling for permanent magnets is available in western countries at the moment .'}]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1_abstract_summary[0]['summary_text'], s2_abstract_summary[0]['summary_text'] #class str"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "(' Hybrid electric vehicles are assumed to play a major role in future mobility concepts . Little emphasis has been laid on the recycling of some key components such as power electronics or electric motors . No industrial recycling for permanent magnets is available in western countries at the moment .',\n ' There has been made much reseacrh regarding the use of photovoltaic sytems in meeting the energy demand in housing and industry . However, there is need for more research . In this study, for environments which have different lighting levels due to daylight factor, there has been proposed a low-cost PV (photovoltaics) based and distributed sensor smart LED illuminating system .')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ex 1."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrase = tokenizer(s1_abstract, s1_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "\r\n",
        "paraphrase = tokenizer(s1_kw, s1_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 73%\n",
            "is paraphrase: 27%\n",
            "not paraphrase: 95%\n",
            "is paraphrase: 5%\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract and summary1: increased paraphraze prob but only to 27%  \r\n",
        "Kws and summary1: didn't work"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrase = tokenizer(s1_abstract, s2_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Shouldn't be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "\r\n",
        "paraphrase = tokenizer(s1_kw, s2_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Shouldn't be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n",
            "not paraphrase: 95%\n",
            "is paraphrase: 5%\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract and summary2: high not paraphrase, good  \r\n",
        "Kws and summary2: high not paraphrase, but as low as previous\r\n",
        "\r\n",
        "Ex 2."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrase = tokenizer(s2_abstract, s2_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "\r\n",
        "paraphrase = tokenizer(s2_kw, s2_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 6%\n",
            "is paraphrase: 94%\n",
            "not paraphrase: 95%\n",
            "is paraphrase: 5%\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract and summary2: high paraphraze prob, good  \r\n",
        "Kws and summary2: didn't work"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrase = tokenizer(s2_abstract, s1_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Shouldn't be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "\r\n",
        "paraphrase = tokenizer(s2_kw, s1_abstract_summary[0]['summary_text'], return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "\r\n",
        "# Shouldn't be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n",
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract and summary1: high not paraphrase, good  \r\n",
        "Kws and summary1: high not paraphrase, but as low as previous\r\n",
        "\r\n",
        "Findings: Summary of abstract may work.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "azureml_py36_transformers",
      "language": "python",
      "display_name": "azureml_py36_transformers"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py36_transformers"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}