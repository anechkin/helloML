{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#meta 3/16/2021 Transformer Models\r\n",
        "#history\r\n",
        "#3/16/2021 TRANSFORMERS\r\n",
        "#      Create an env\r\n",
        "#      Try huggingfac"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029716418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline \r\n",
        "print(pipeline('sentiment-analysis')('we love you'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029727338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline('sentiment-analysis')('jeans'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.5292837023735046}]\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029733057
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2Model\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\n",
        "model = TFGPT2Model.from_pretrained('gpt2')\r\n",
        "text = \"Replace me by any text you'd like.\"\r\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\r\n",
        "output = model(encoded_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1616029770687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Classification\r\n",
        "https://huggingface.co/transformers/task_summary.html"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\r\n",
        "import tensorflow as tf\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_0 = \"The company HuggingFace is based in New York City\"\r\n",
        "sequence_1 = \"Apples are especially bad for your health\"\r\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\r\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n",
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029976763
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_0 = \"blue jeans\"\r\n",
        "sequence_1 = \"red apples\"\r\n",
        "sequence_2 = \"pants\"\r\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\r\n",
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 41%\n",
            "is paraphrase: 59%\n",
            "not paraphrase: 78%\n",
            "is paraphrase: 22%\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616029930517
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "azureml_py36_transformers",
      "language": "python",
      "display_name": "azureml_py36_transformers"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py36_transformers"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}